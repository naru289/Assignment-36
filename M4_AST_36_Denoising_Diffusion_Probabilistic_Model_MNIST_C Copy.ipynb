{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbQzNNtS_iWt"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment: Implementation of Denoising Diffusion Probabilistic Models (DDPM)"
      ],
      "id": "tbQzNNtS_iWt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCgQO0zI_wQN"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "uCgQO0zI_wQN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOd1dGbK_zCq"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* understand diffusion models\n",
        "* implement Denoising Diffusion Probabilistic Models (DDPM)"
      ],
      "id": "kOd1dGbK_zCq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gKCL5tRk5CP"
      },
      "source": [
        "## Dataset"
      ],
      "id": "9gKCL5tRk5CP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RMO3CTNk8pl"
      },
      "source": [
        "\n",
        "\n",
        "### Description\n",
        "\n",
        "\n",
        "1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples,\n",
        "which means each digit occurs 6000 times in the training set and 1000 times in the testing set. (approximately).\n",
        "2. Each image is Size Normalized and Centered\n",
        "3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value.\n",
        "4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n",
        "\n",
        "\n",
        "### History\n",
        "\n",
        "Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90’s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n",
        "\n",
        "Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license.\n",
        "\n",
        "It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students."
      ],
      "id": "2RMO3CTNk8pl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6xDYtrwlDRj"
      },
      "source": [
        "## Domain Information\n"
      ],
      "id": "D6xDYtrwlDRj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyF_hOsQlEvM"
      },
      "source": [
        "\n",
        "Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n",
        "\n",
        "![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n",
        "\n"
      ],
      "id": "tyF_hOsQlEvM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OfFZYKNDE7S"
      },
      "source": [
        "## Information"
      ],
      "id": "9OfFZYKNDE7S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNTMdIBmDNXS"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Generative models create latent representations, which distil information from big data in order to generate realistic and novel data points.  GAN, VAE models have shown great success in generating high-quality samples, but each has some limitations of its own. GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature. VAE relies on a loss function.\n",
        "\n",
        "There is another set of techniques which originate from probabilistic likelihood estimation methods and take inspiration from physical phenomenon; it is called, **Diffusion Models**. The central idea behind Diffusion Models comes from the thermodynamics of gas molecules whereby the molecules diffuse from high density to low density areas. This movement is often referred in physics literature as the increase of entropy or heat death. In information theory, this equates to loss of information due to gradual intervention of noise.\n",
        "\n",
        "\n",
        "The key concept in Diffusion Modelling is that if we could build a learning model which can learn the systematic decay of information due to noise, then it should be possible to reverse the process and therefore, recover the information back from the noise. This concept is similar to VAEs in the way that it tries to optimize an objective function by first projecting the data onto the latent space and then recovering it back to the initial state. However, instead of learning the data distribution, the system aims to model a series of noise distributions in a **Markov Chain** and **“decodes”** the data by **undoing/denoising** the data in a hierarchical fashion.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png\" width=600px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "### What is Diffusion(Denoising) Model?\n",
        "\n",
        "The idea of denoising diffusion model has been around for a long time. It has its roots in Diffusion Maps concept which is one of the dimensionality reduction techniques used in Machine Learning literature. It also borrows concepts from the probabilistic methods such as Markov Chains which has been used in many applications.\n",
        "\n",
        "A denoising diffusion modeling is a two step process: the **forward diffusion process** and the **reverse process** or the reconstruction.\n",
        "\n",
        " * In the forward diffusion process $q$, gaussian noise is introduced successively until the data becomes all noise.\n",
        "\n",
        " * The reverse/ reconstruction process $p_{θ}$, undoes the noise by learning the conditional probability densities using a neural network model\n",
        "\n",
        "Both the forward and reverse process indexed by $t$ happen for some number of finite time steps $T$ (the DDPM authors use T=1000). We start with $t=0$ where we sample a real image $x_{0}$ from your data distribution (let's say an image of a celebrity from CelebA dataset), and the forward process samples some noise from a Gaussian distribution at each time step $t$, which is added to the image of the previous time step. Given a sufficiently large $T$ and a well behaved schedule for adding noise at each time step, we end up with what is called an isotropic Gaussian distribution at $t=T$ via a gradual process.\n",
        "\n"
      ],
      "id": "WNTMdIBmDNXS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ],
      "id": "BNLA8HiKxQhc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": [],
      "id": "xWMVQWk58aXm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": [],
      "id": "cwqosl928dBA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vInRrxGKIu64",
        "outputId": "4b6a375b-5f63-4c86-cd43-1603a372363b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_36_Denoising_Diffusion_Probabilistic_Model_MNIST_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/mnist_model.pth\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=2906\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "id": "vInRrxGKIu64"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkRbFCVAQfV7"
      },
      "source": [
        "### Importing required packages"
      ],
      "id": "EkRbFCVAQfV7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b18a40",
      "metadata": {
        "id": "b5b18a40"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbm-8HxIqvxR"
      },
      "source": [
        "### Initializing CUDA\n",
        "\n",
        "CUDA is used as an interface between our code and the GPU.\n",
        "\n",
        "Normally, we run the code in the CPU. To run it in the GPU, we need CUDA. Check if CUDA is available:"
      ],
      "id": "vbm-8HxIqvxR"
    },
    {
      "cell_type": "code",
      "source": [
        "# To test whether GPU instance is present in the system of not.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)"
      ],
      "metadata": {
        "id": "oiDSxSvQTpax"
      },
      "id": "oiDSxSvQTpax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuGyWSz8q4NQ"
      },
      "source": [
        "If it's False, then we run the program on CPU. If it's True, then we run the program on GPU.\n",
        "\n",
        "Let us initialize some GPU-related variables:"
      ],
      "id": "DuGyWSz8q4NQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_WeWksDqvxb"
      },
      "source": [
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "m_WeWksDqvxb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG_Q01fTqvxl"
      },
      "source": [
        "### Load MNIST data\n",
        "\n",
        "Now, we'll load the MNIST data. For the first time, we may have to download the data, which can take a while.\n",
        "\n",
        "Now,\n",
        "\n",
        "* We will load both the training set and the testing sets\n",
        "\n",
        "* We will use  transform.compose() to convert the datasets into tensors using transforms.ToTensor(). We also normalize them by setting the mean and standard deviation using transforms.Normalize().\n"
      ],
      "id": "nG_Q01fTqvxl"
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 32\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "Nt9cVkkvUFW_"
      },
      "id": "Nt9cVkkvUFW_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), # Resizes image\n",
        "                                transforms.ToTensor(), # Scales data to [0,1]\n",
        "                                transforms.Lambda(lambda x: (x * 2) - 1)]) # Rescale data to [-1, 1] range"
      ],
      "metadata": {
        "id": "Hx44Gla3UJGx"
      },
      "id": "Hx44Gla3UJGx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the train and set file\n",
        "train_dataset = datasets.MNIST('../data', train=True, transform=transform, download=True) # 60000 images\n",
        "test_dataset = datasets.MNIST('../data', train=False, transform=transform, download=True) # 10000 images\n",
        "\n",
        "# Subset of dataset\n",
        "# train = torch.utils.data.Subset(train, indices=range(len(train)//100))\n",
        "# test =torch.utils.data. Subset(test, indices=range(len(test)//100))\n",
        "\n",
        "# Combining train and test sets\n",
        "data = torch.utils.data.ConcatDataset([train_dataset, test_dataset])"
      ],
      "metadata": {
        "id": "kv7HuGTLUHHa"
      },
      "id": "kv7HuGTLUHHa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training samples\n",
        "len(train_dataset)"
      ],
      "metadata": {
        "id": "f_uTMAdWUZw3"
      },
      "id": "f_uTMAdWUZw3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of one training image\n",
        "train_dataset[0][0].size()"
      ],
      "metadata": {
        "id": "sgNh2f84Udlp"
      },
      "id": "sgNh2f84Udlp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AsNZKmR7VwV"
      },
      "source": [
        "\n",
        "\n",
        "**torch.utils.data.DataLoader** class represents a Python iterable over a dataset, with following features.\n",
        "\n",
        "1. Batching the data\n",
        "2. Shuffling the data\n",
        "3. Load the data in parallel using multiprocessing workers.\n",
        "\n",
        "\n",
        "The batches of train and test data are provided via data loaders that provide iterators over the datasets to train our models."
      ],
      "id": "1AsNZKmR7VwV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde5ae14",
      "metadata": {
        "id": "bde5ae14"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "print(len(data_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWYm5tj4qvxs"
      },
      "source": [
        "The train and test data are provided via data loaders that provide iterators over the datasets.\n",
        "\n",
        "The first element of training data (X_train) is a 4th-order tensor of size (batch_size, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels where '1' represents one input image channel i.e. grey scale. y_train is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
      ],
      "id": "FWYm5tj4qvxs"
    },
    {
      "cell_type": "code",
      "source": [
        "for (X_train, y_train) in data_loader:\n",
        "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
        "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
        "    break"
      ],
      "metadata": {
        "id": "N-7GBvtL1mHO"
      },
      "id": "N-7GBvtL1mHO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g00u_5njqvx0"
      },
      "source": [
        "#### Plotting the  first 10 training digit images"
      ],
      "id": "g00u_5njqvx0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7763f20f",
      "metadata": {
        "id": "7763f20f"
      },
      "outputs": [],
      "source": [
        "pltsize=2\n",
        "plt.figure(figsize=(15*pltsize, pltsize))\n",
        "\n",
        "for i in range(10):\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(X_train[i,:,:,:].numpy().reshape(32,32), cmap=\"gray\")\n",
        "    plt.title('Class: '+str(y_train[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff54c503",
      "metadata": {
        "id": "ff54c503"
      },
      "source": [
        "### Building the Diffusion Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mi-6NmvVi9s"
      },
      "source": [
        "The basic idea behind diffusion models is rather simple. They take the input image $\\mathbf{x}_0$ and gradually add Gaussian noise to it through a series of $T$ steps. We will call this the forward process. Notably, this is unrelated to the forward pass of a neural network. This part is necessary to generate the targets for our neural network (the image after applying $t<T$ noise steps).\n",
        "\n",
        "Afterward, a neural network is trained to recover the original data by reversing the noising process. By being able to model the reverse process, we can generate new data. This is the so-called reverse diffusion process or, in general, the sampling process of a generative model.\n",
        "\n",
        "#### Step 1: The forward difussion process = Noise scheduler\n",
        "\n",
        "We first need to build the inputs for our model, which are more and more noisy images. Instead of doing this sequentially, we can use the closed form provided in the papers to calculate the image for any of the timesteps individually.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://theaisummer.com/static/1f5f940d6d3f1e00b3777066f6695331/073e9/forward-diffusion.png\" width=650px/>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "**In more mathematical form**\n",
        "\n",
        "We can formally define the forward diffusion process as a Markov Chain\n",
        "of $T$ steps. Here, a Markov chain means that each step only depends on the previous one, which is a mild assumption and therefore, unlike an encoder in the VAEs it doesn’t require a training.\n",
        "\n",
        "Let $q(x_{0})$ be the real data distribution, say of \"real images\". We can sample from this distribution to get an image, $x_{0} ∼ q(x_{0})$. We define the forward diffusion process $q(x_{t}∣x_{t−1})$ which adds Gaussian noise at each time step $t$, according to a known variance schedule $0 < β_{1} <β_{2} <...<β_{T}​<1$  and $x_{t-1}$ is the previous less noise image which means the mean of our distribution is exactly the previous image multiplied with this term that depends on the variance schedule $β$, the variance of this normal distribution is fixed to $β$ multiplied with identity $I$ as\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "$\\quad q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I})$\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Recall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a mean $μ$ and a variance $σ^{2}≥ 0$. Basically, each new (slightly noisier) image at time step $t$ is drawn from a **conditional Gaussian distribution** with mean $μ_{t} = \\sqrt{1−β_{t}}x_{t−1}$​\n",
        "​and variance $σ_{t}^2 = β_{t}$ ​, which we can do by sampling $ϵ∼N(0,I)$ and then setting $x_{t}=\\sqrt{1−β_{t}}x_{t−1} + \\sqrt{β_{t}}​ϵ$​.\n",
        "\n",
        "\n",
        "Note that the $β_{t}$​ aren't constant at each time step $t$  --- in fact one defines a so-called \"variance schedule\", which can be linear, quadratic, cosine, etc. as we will see further (a bit like a learning rate schedule).\n",
        "\n",
        "So starting from $x_{0}$​, we end up with $x_{1}$,...,$x_{t}$,...,$x_{T}$, where $x_{T}$​ is pure Gaussian noise if we set the schedule appropriately.\n",
        "\n",
        "\n",
        "So we need a neural network to represent a (conditional) probability distribution of the backward process. If we assume this reverse process is Gaussian as well, then recall that any Gaussian distribution is defined by 2 parameters:\n",
        "* a mean parametrized by \\\\(\\mu_\\theta\\\\);\n",
        "* a variance parametrized by \\\\(\\Sigma_\\theta\\\\);\n",
        "\n",
        "so we can parametrize the process as :\n",
        "<center>\n",
        "$$ p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_{t},t), \\Sigma_\\theta (\\mathbf{x}_{t},t))$$\n",
        "</center>\n",
        "<br>\n",
        "where the mean and variance are also conditioned on the noise level \\\\(t\\\\).\n",
        "\n",
        "\n",
        "Hence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to **keep the variance fixed, and let the neural network only learn (represent) the mean \\\\(\\mu_\\theta\\\\) of this conditional probability distribution**. From the paper:\n",
        "\n",
        "> First, we set \\\\(\\Sigma_\\theta ( \\mathbf{x}_t, t) = \\sigma^2_t \\mathbf{I}\\\\) to untrained time dependent constants. Experimentally, both \\\\(\\sigma^2_t = \\beta_t\\\\) and \\\\(\\sigma^2_t  = \\tilde{\\beta}_t\\\\) (see paper) had similar results.\n",
        "\n",
        "So we continue, assuming that our neural network only needs to learn/represent the mean of this conditional probability distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note:**\n",
        "\n",
        "1. Refer to the folowing [paper](https://arxiv.org/pdf/2006.11239.pdf) to understand more about the diffusion models.\n",
        "\n",
        "2. Refer to the following [paper](https://arxiv.org/pdf/2208.11970.pdf) for Understanding Diffusion Models."
      ],
      "id": "1Mi-6NmvVi9s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8CJJc5IBKrC"
      },
      "source": [
        "#### Defining an objective function (by reparametrizing the mean)\n",
        "\n",
        "\n",
        "To derive an objective function to learn the mean of the backward process, the authors observe that the combination of \\\\(q\\\\) and \\\\(p_\\theta\\\\) can be seen as a variational auto-encoder (VAE). Hence, the **variational lower bound** (also called ELBO) can be used to minimize the negative log-likelihood with respect to ground truth data sample \\\\(\\mathbf{x}_0\\\\) (we refer to the VAE paper for details regarding ELBO). It turns out that the ELBO for this process is a sum of losses at each time step \\\\(t\\\\), \\\\(L = L_0 + L_1 + ... + L_T\\\\).\n",
        "\n",
        "A direct consequence of the constructed forward process \\\\(q\\\\), is that we can sample \\\\(\\mathbf{x}_t\\\\) at any arbitrary noise level conditioned on \\\\(\\mathbf{x}_0\\\\) (since sums of Gaussians is also Gaussian). This is very convenient:  we don't need to apply \\\\(q\\\\) repeatedly in order to sample \\\\(\\mathbf{x}_t\\\\).\n",
        "We have that\n",
        "$$q(\\mathbf{x}_t | \\mathbf{x}_0) = \\cal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1- \\bar{\\alpha}_t) \\mathbf{I})$$\n",
        "\n",
        "with \\\\(\\alpha_t := 1 - \\beta_t\\\\) and \\\\(\\bar{\\alpha}t := \\Pi_{s=1}^{t} \\alpha_s\\\\). Let's refer to this equation as the \"nice property\". This means we can sample Gaussian noise and scale it appropriatly and add it to \\\\(\\mathbf{x}_0\\\\) to get \\\\(\\mathbf{x}_t\\\\) directly. Note that the \\\\(\\bar{\\alpha}_t\\\\) are functions of the known \\\\(\\beta_t\\\\) variance schedule and thus are also known and can be precomputed. This then allows us, during training, to **optimize random terms of the loss function \\\\(L\\\\)** (or in other words, to randomly sample \\\\(t\\\\) during training and optimize \\\\(L_t\\\\)."
      ],
      "id": "I8CJJc5IBKrC"
    },
    {
      "cell_type": "markdown",
      "id": "96605248",
      "metadata": {
        "id": "96605248"
      },
      "source": [
        "#### Variance schedule\n",
        "\n",
        "The noise added at each timestep `t` is determined by the variance schedule `beta_t`. There are different kinds of variance schedule - linear, cosine, quadratic, etc that are fixed (not learned by the model). These variances can also be learned by the model.\n",
        "\n",
        "A noisy image that has isotropic gaussian distribution at the final timestep `t = T` can be achieved by setting `t` to a large number and using proper variance schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0946b523",
      "metadata": {
        "id": "0946b523"
      },
      "outputs": [],
      "source": [
        "# The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps T.\n",
        "# This happens according to a variance schedule. The original DDPM authors employed a linear schedule:\n",
        "# We set the forward process variances to constants increasing linearly from β1=10^−4 to βT=0.02\n",
        "def linear_variance_schedule(T):\n",
        "    \"\"\"\n",
        "    Linear variance schedule from https://arxiv.org/abs/2006.11239\n",
        "\n",
        "    params: T: number of time steps (int)\n",
        "\n",
        "    Returns: beta (variance schedule) of length T\n",
        "\n",
        "    \"\"\"\n",
        "    ### torch.linspace(start, stop, num,...) method outputs **num** values from **start** to **stop**\n",
        "    return torch.linspace(0.0001, 0.02, T).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24pQMkcPaEzv"
      },
      "source": [
        "To start with, let's use the linear schedule for  T=1000  time steps and define the various variables from the  $β_{t}$  which we will need, such as the cumulative product of the variances  $\\bar{α}_{t}$  . Each of the variables below are just 1-dimensional tensors, storing values from  t  to  T ."
      ],
      "id": "24pQMkcPaEzv"
    },
    {
      "cell_type": "markdown",
      "id": "01cc190e",
      "metadata": {
        "id": "01cc190e"
      },
      "source": [
        "Forward diffusion process: Corrupting image with noise at each timestep `t` (controlled by variance schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72889b5c",
      "metadata": {
        "id": "72889b5c"
      },
      "outputs": [],
      "source": [
        "def forward_diffusion(x0, t):\n",
        "    \"\"\"\n",
        "    Math based on https://arxiv.org/abs/2006.11239\n",
        "\n",
        "    :params x0: Input image - shape(batch_size, num_channels, img_size, img_size)\n",
        "    :params t: time step - shape(batch_size)\n",
        "    :params schedule: Variance schedule - 'linear' or 'cosine'\n",
        "\n",
        "    Returns image corrupted with noise at a given timestep\n",
        "    along with the noise added at that time step\n",
        "    \"\"\"\n",
        "    dim = len(x0.shape)-1\n",
        "\n",
        "    # Noise to be added to image\n",
        "    noise = torch.randn_like(x0)\n",
        "\n",
        "    # Variance scheduler\n",
        "    beta = linear_variance_schedule(T)\n",
        "\n",
        "    # Pre-calculate different terms for closed form\n",
        "    # Define alphas\n",
        "    alpha = 1 - beta\n",
        "\n",
        "    # The cumprod method, is a cummulative product function.i.e.\n",
        "    # if x = [1,2,3,4], then torch.cumprod(x) = [1,1*2,1*2*3,1*2*3*4]\n",
        "    alpha_bar = torch.cumprod(alpha, 0)\n",
        "\n",
        "    # calculations for forward diffusion q(x_t | x_{t-1}) and others\n",
        "    sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
        "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar)\n",
        "\n",
        "    # mean + variance\n",
        "    # Calculate the new sample based on the original input + noise which gives noisified version of an image\n",
        "    # at a specific time step\n",
        "    x = (sqrt_alpha_bar[t].reshape(len(t), *(1,)*dim).to(device) * x0.to(device)) +\\\n",
        "        (sqrt_one_minus_alpha_bar[t].reshape(len(t), *(1,)*dim).to(device) * noise.to(device))\n",
        "\n",
        "    return x.to(device), noise.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2402b1",
      "metadata": {
        "id": "4f2402b1"
      },
      "source": [
        "Forward diffusion using Linear variance schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47512dde",
      "metadata": {
        "id": "47512dde"
      },
      "outputs": [],
      "source": [
        "# No of time steps\n",
        "T = 300\n",
        "x0 = train_dataset[20][0]\n",
        "plt.figure(figsize=(55,55))\n",
        "\n",
        "print('Visualizing how noise changes the original image as time progresses using linear variance schedule')\n",
        "samples = []\n",
        "for i in range(T):\n",
        "    num_images = 10\n",
        "    step_size = int(T/num_images)\n",
        "    x, _ = forward_diffusion(x0, torch.tensor(i).view(1,-1))\n",
        "    samples.append(x)\n",
        "    if i % step_size == 0:\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1, num_images, int(i//step_size+1))\n",
        "        plt.imshow(x.detach().cpu().numpy().reshape(32,32), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk7bpeARDZoP"
      },
      "source": [
        "## UNet Model\n",
        "\n",
        "A simple Unet is trained to predict the noise added to image(s) at a timestep `t`. A Unet architecture is used since the dimensions of the input (image) and the output (noise) are the same.\n",
        "\n",
        "The model parameters are shared across time to distinguish the noise added at each timestep `t`. Sinusoidal positional embeddings inspired by Transformer  are used to help the network know at what timestep `t` it is operating.\n",
        "\n",
        "What is typically used here is very similar to that of an Autoencoder.Autoencoders have a \"bottleneck\" layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the \"bottleneck\", and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.\n",
        "\n",
        "In terms of architecture, the DDPM authors went for a **U-Net**. This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder.\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1_Hej_VTgdUWGsxxIuyZACCGjpbCGIUi6\" width=\"570\" />\n",
        "</p>\n",
        "\n",
        "As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.\n",
        "\n",
        "Below, we implement this network, step-by-step."
      ],
      "id": "Hk7bpeARDZoP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK4stcL6_DBO"
      },
      "source": [
        "#### Step 2: The reverse diffusion process = UNet\n",
        "\n",
        "<br>\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://theaisummer.com/static/9bb372bb74034360fe7891d546e3c5b4/01dae/reverse-diffusion.png\" width=\"570\" />\n",
        "</p>\n",
        "<br>\n",
        "\n"
      ],
      "id": "bK4stcL6_DBO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCHVAr00EhlZ"
      },
      "source": [
        "**Sinusoidal Position embeddings**\n",
        "\n",
        "As the parameters of the neural network are shared across time (noise level) which means it can't distinguish between the different time steps, that means it needs to filter out the noise from images with very different noise intensities, for this the authors employ sinusoidal position embeddings to encode $t$, inspired by the Transformer model they are a clever way to encode discrete positional information like sequence steps. This makes the neural network \"know\" at which particular time step (noise level) it is operating, for every image in a batch.\n",
        "\n",
        "But the amount of noise present in the input is dependent on the timestep. This means that passing the timestep into the model is a logical step. In every residual block, we insert a time embedding (obtained from passing the timestep into a Sinusoidal Positional Embedding function). This embedding encodes the time into a higher dimension which can be added to the residual block.\n",
        "\n",
        "The SinusoidalPositionEmbeddings module takes a tensor of shape (batch_size, 1) as input (i.e. the noise levels of several noisy images in a batch), and turns this into a tensor of shape (batch_size, dim), with dim being the dimensionality of the position embeddings. This is then added to each residual block, as we will see further."
      ],
      "id": "QCHVAr00EhlZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f8a368",
      "metadata": {
        "id": "10f8a368"
      },
      "outputs": [],
      "source": [
        "def positional_encodings(t, dim):\n",
        "    \"\"\"\n",
        "    To enable the model to learn the noise added at each timestep,\n",
        "    a positional encoding along with the input is fed as input\n",
        "\n",
        "    Reference - https://huggingface.co/blog/annotated-diffusion#position-embeddings\n",
        "\n",
        "    :params t: time steps - a tensor\n",
        "    :params dim: encodings dimension - a number (int)\n",
        "\n",
        "    Returns embeddings - shape(len(t), dim)\n",
        "    \"\"\"\n",
        "    half_dim = dim // 2\n",
        "    embeddings = math.log(10000) / (half_dim)\n",
        "    embeddings = torch.exp(torch.arange(half_dim) * -embeddings)\n",
        "    embeddings = t[:, None] * embeddings[None, :].to(device)\n",
        "    # Embeddings are calculated using sine and cosine functions for a specific embedding size defined by dim in this case\n",
        "    embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "    # For alternating sine and cosine embeddings\n",
        "    # Ref - https://stackoverflow.com/questions/71628542/how-to-alternatively-concatenate-pytorch-tensors\n",
        "    embeddings = embeddings.T.flatten()\n",
        "    embeddings = torch.stack(torch.split(embeddings, len(t)), dim=1).reshape(len(t),-1)\n",
        "\n",
        "    return embeddings # Returns a vector that describes the position of an index in a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ursmjC8M8Jh"
      },
      "source": [
        "In the below class we define the convolutional blocks of the UNet Model"
      ],
      "id": "4ursmjC8M8Jh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da8a90d",
      "metadata": {
        "id": "4da8a90d"
      },
      "outputs": [],
      "source": [
        "class Block(nn.ModuleDict):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super(Block, self).__init__()\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "        if up: # Upsampling (Decoder)\n",
        "            self.conv2d1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1) # Same size, half the no. of channels\n",
        "            self.conv2d2 = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1) # Double size, same no. of channels\n",
        "            self.conv2dl = nn.Conv2d(out_ch, out_ch, 3, padding=1) # Same size, same no. of channels\n",
        "        else: # Downsampling (Encoder)\n",
        "            self.conv2d1 = nn.Conv2d(in_ch, out_ch, 3, padding=1) # Same size, double the no. of channels\n",
        "            self.conv2d2 = nn.Conv2d(out_ch, out_ch, 3, padding=1) # Same size, same no. of channels\n",
        "            self.conv2dl = nn.Conv2d(out_ch, out_ch, 4, 2, 1) # Half size, same no. of channels\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2d3 = torch.nn.Conv2d(out_ch, out_ch, 3, padding=1) # Same size, same no. of channels\n",
        "        self.conv2d4 = torch.nn.Conv2d(out_ch, out_ch, 3, padding=1) # Same size, same no. of channels\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Block performs a series of convolutions that increase/decrease\n",
        "        the spatial dimension and the number of channels in both\n",
        "        downsampling (Encoder) and upsampling (Decoder) stages\n",
        "\n",
        "        :params x: input\n",
        "        :params t: time embeddings\n",
        "        \"\"\"\n",
        "        c1 = self.bnorm1(self.relu(self.conv2d1(x)))\n",
        "\n",
        "        # Time embedding - 2D\n",
        "        t_emb = self.relu(self.time_mlp(t)) # shape(batch_size, out_ch)\n",
        "\n",
        "        # Extend time embedding to 4D like x\n",
        "        t_emb = t_emb.unsqueeze(2).unsqueeze(3) # shape(batch_size, out_ch, 1, 1)\n",
        "\n",
        "        c2 = self.relu(self.conv2d2(c1)) # shape(batch_size, out_ch, size, size)\n",
        "\n",
        "        # Add time embedding to up-convolved input\n",
        "        c2 = c2 + t_emb # shape(batch_size, out_ch, size, size)\n",
        "\n",
        "        c3 = self.bnorm2(self.relu(self.conv2d3(c2)))\n",
        "        c4 = self.relu(self.conv2d4(c3))\n",
        "        cl = self.conv2dl(c4)\n",
        "\n",
        "        return cl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Encoder class"
      ],
      "metadata": {
        "id": "azyR1kYtba2y"
      },
      "id": "azyR1kYtba2y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307293ae",
      "metadata": {
        "id": "307293ae"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, channels=(64, 128, 256, 512, 1024), time_emb_dim=32):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([Block(channels[c], channels[c+1],\n",
        "                                                   time_emb_dim) for c in range(len(channels)-1)])\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Encoder doubles the number of channels in the first convolution,\n",
        "        followed by a series of convolutions that neither change size nor no. of channels,\n",
        "        halves the input size (spatial dimension) at the final convolution\n",
        "        and saves the outputs to concatenate during upsampling.\n",
        "        It is repeated until the number of channels reaches max. no. of channels\n",
        "\n",
        "        :params x: input, shape(batch_size, enc[0], original i/p size, original i/p size)\n",
        "        :params t: time embeddings, shape(batch_size, self.tim_dim)\n",
        "\n",
        "        Returns output, shape(batch_size, enc[-1], size/len(enc), size/len(enc)) and a list of encoder o/ps\n",
        "        \"\"\"\n",
        "        enc_ops = []\n",
        "        for b, block in enumerate(self.encoder_blocks):\n",
        "            x = block(x, t)\n",
        "            enc_ops.append(x) # Save encoder o/p\n",
        "\n",
        "        return x, enc_ops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Decoder class"
      ],
      "metadata": {
        "id": "Zwi4zlM9biQm"
      },
      "id": "Zwi4zlM9biQm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51483367",
      "metadata": {
        "id": "51483367"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, channels=(1024, 512, 256, 128, 64), time_emb_dim=32):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.decoder_blocks = nn.ModuleList([Block(channels[i], channels[i+1],\n",
        "                                                   time_emb_dim, up=True) for i in range(len(channels)-1)])\n",
        "\n",
        "    def forward(self, x, enc_ops, t):\n",
        "        \"\"\"\n",
        "        Decoder halves the number of channels in the first convolution step,\n",
        "        doubles the input size in the second convolution step,\n",
        "        followed by a series of convolutions that neither change size nor no. of channels.\n",
        "        It is repeated until the number of channels reaches min. no. of channels\n",
        "\n",
        "        :params x: final encoder output, shape(batch_size, enc[-1], size/len(enc), size/len(enc))\n",
        "        :params enc_ops: list of encoder outputs\n",
        "        :params t: time embeddings, shape(batch_size, self.tim_dim)\n",
        "\n",
        "        Returns output, shape(batch_size, dec[-1], original i/p size, original i/p size)\n",
        "        \"\"\"\n",
        "        for i in range(len(self.channels)-1):\n",
        "            enc = enc_ops[i]\n",
        "            x = torch.cat((enc, x), 1) # Concatenate encoder o/p (on channels dim, so doubles the no. of channels)\n",
        "            x = self.decoder_blocks[i](x, t)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNet Model"
      ],
      "metadata": {
        "id": "_yex2IPabrCJ"
      },
      "id": "_yex2IPabrCJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f797291",
      "metadata": {
        "id": "6f797291"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Unet, self).__init__()\n",
        "        enc_ch=(64, 128, 256, 512, 1024) # Encoder channels\n",
        "        dec_ch=(1024, 512, 256, 128, 64) # Decoder channels\n",
        "        image_channels = 1 # Number of i/p channels\n",
        "        self.tim_dim = 256 # time embedding dimension\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "                                      nn.Linear(self.tim_dim, self.tim_dim),\n",
        "                                      nn.ReLU()\n",
        "                                      )\n",
        "\n",
        "        self.conv0 = nn.Conv2d(image_channels, enc_ch[0], 3, padding=1) # 3 channels to enc[0] channels\n",
        "        self.encoder = Encoder(channels=enc_ch, time_emb_dim=self.tim_dim)\n",
        "        self.decoder = Decoder(channels=dec_ch, time_emb_dim=self.tim_dim)\n",
        "        self.conv1 = nn.Conv2d(dec_ch[-1], image_channels, 3, padding=1) # dec[-1] channels to 3 channels\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        \"\"\"\n",
        "        Initial convolution - 3 channels to enc[0] channels,\n",
        "        followed by encoder and decoder operations and\n",
        "        final convolution - dec[-1] channels to 3 channels\n",
        "\n",
        "        :params x: images at specific time steps, with(t=1toT)/without(t=0) noise added,\n",
        "                   shape(batch_size, channels, img_size, img_size)\n",
        "        :params timestep: a tensor of time steps, shape(batch_size)\n",
        "\n",
        "        Returns noise (predicted) at specific timesteps shape(x)\n",
        "        \"\"\"\n",
        "        # Time embeddings\n",
        "        t = positional_encodings(timestep, self.tim_dim)\n",
        "\n",
        "        t = self.time_mlp(t) # shape(batch_size, self.tim_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        x = self.conv0(x)\n",
        "\n",
        "        # Encoder / down-sampling\n",
        "        res, enc_ops = self.encoder(x, t)\n",
        "\n",
        "        # Decoder / Up-sampling\n",
        "        out = self.decoder(res, enc_ops[::-1], t)\n",
        "\n",
        "        # Final convolution - Set output channels to desired number\n",
        "        out = self.conv1(out)\n",
        "\n",
        "        return out # shape(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the instance for the model\n",
        "unet = Unet()\n",
        "print(\"Num params: \", sum(p.numel() for p in unet.parameters()))"
      ],
      "metadata": {
        "id": "CR2fhLXBcL8Q"
      },
      "id": "CR2fhLXBcL8Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b66d3b99",
      "metadata": {
        "id": "b66d3b99"
      },
      "source": [
        "#### Create an instance for the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unet = Unet().to(device)\n",
        "print(unet)"
      ],
      "metadata": {
        "id": "YLcSJDaY4D2d"
      },
      "id": "YLcSJDaY4D2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_R4r9gGQqVF"
      },
      "source": [
        "\n",
        "**Further improvements to the model can be implemented:**\n",
        "- By adding Residual connections\n",
        "- Different activation functions like SiLU, GWLU, ...\n",
        "- GroupNormalization\n",
        "- Attention\n"
      ],
      "id": "J_R4r9gGQqVF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the model and the optimizer\n",
        "# def save_model(model, optimizer):\n",
        "#     \"\"\"\n",
        "#     Function to save the trained model to disk.\n",
        "#     \"\"\"\n",
        "#     print(f\"Saving final model...\")\n",
        "#     torch.save({\n",
        "#                 'model_state_dict': model.state_dict(),\n",
        "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                 }, 'outputs/final_model.pth')"
      ],
      "metadata": {
        "id": "DJ4tQ5bKBO7j"
      },
      "id": "DJ4tQ5bKBO7j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f76f69e9",
      "metadata": {
        "id": "f76f69e9"
      },
      "source": [
        "## Training the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMb9eJ9dCSKU"
      },
      "source": [
        "Here, \\\\(\\mathbf{x}_0\\\\) is the initial (real, uncorruped) image, and we see the direct noise level \\\\(t\\\\) sample given by the fixed forward process. \\\\(\\mathbf{\\epsilon}\\\\) is the pure noise sampled at time step \\\\(t\\\\), and \\\\(\\mathbf{\\epsilon}_\\theta (\\mathbf{x}_t, t)\\\\) is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\n",
        "\n",
        "The training algorithm now looks as follows:\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1LJsdkZ3i1J32lmi9ONMqKFg5LMtpSfT4\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "In other words:\n",
        "* we take a random sample $\\mathbf{x}_0$ from the real unknown and possibily complex data distribution $q(\\mathbf{x}_0)$\n",
        "* we sample a noise level $t$ uniformally between $1$ and $T$ (i.e., a random time step)\n",
        "* we sample some noise from a Gaussian distribution and corrupt the input by this noise at level $t$ using the nice property defined above\n",
        "* the neural network is trained to predict this noise based on the corruped image $\\mathbf{x}_t$, i.e. noise applied on $\\mathbf{x}_0$ based on known schedule $\\beta_t$\n",
        "\n"
      ],
      "id": "yMb9eJ9dCSKU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "LR = 1e-4\n",
        "\n",
        "# Adam Optimizer\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "z2qx6OV7BUWb"
      },
      "id": "z2qx6OV7BUWb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8613dc",
      "metadata": {
        "id": "8e8613dc"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 4\n",
        "T = 1000\n",
        "print(T)\n",
        "for epoch in range(EPOCHS):\n",
        "    # Iterate though the train loader\n",
        "    for step, batch in enumerate(data_loader):\n",
        "\n",
        "        # Zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # sample t uniformally for every example in the batch\n",
        "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long().to(device)\n",
        "\n",
        "        # Forward Diffusion\n",
        "        x_noisy , true_noise = forward_diffusion(batch[0].to(device), t)\n",
        "\n",
        "        # Backward Diffusion\n",
        "        predicted_noise = unet(x_noisy.to(device), t)\n",
        "\n",
        "        # Compute the loss\n",
        "        # loss function: While large models tend to use mean squared error (MSE) loss,\n",
        "        # MSE loss generates more diverse samples while MAE loss leads to smoother images\n",
        "        loss = F.l1_loss(predicted_noise, true_noise)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0 and step == 0:\n",
        "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ea5adc",
      "metadata": {
        "id": "a9ea5adc"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS1C2EtPT5J3"
      },
      "source": [
        "\n",
        "\n",
        "Without adding @torch.no_grad() we quickly run out of memory, because pytorch tracks all the previous images for gradient calculation\n",
        "Because we pre-calculated the noise variances for the forward pass, we also have to use them when we sequentially perform the backward process.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1ij80f8TNBDzpKtqHjk_sh8o5aby3lmD7\" width=\"500\" />\n",
        "</center>\n",
        "\n",
        "Generating new images from a diffusion model happens by reversing the diffusion process: we start from $T$, where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step $t=0$. As shown above, we can derive a slighly less denoised image $x_{t−1}$ by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.\n",
        "\n",
        "Ideally, we end up with an image that looks like it came from the real data distribution or close to it."
      ],
      "id": "JS1C2EtPT5J3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0905f4bf",
      "metadata": {
        "id": "0905f4bf"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sampling(x_t, t, unet):\n",
        "    \"\"\"\n",
        "    Uses trained model to gradually denoise the noisy image (at t) and\n",
        "    recover image (at t-1)\n",
        "\n",
        "    :params xt: noisy image at timestep t # shape(batch_size, channels, img_size, img_size)\n",
        "    :params t: timesteps, a tensor # shape(batchsize)\n",
        "    :params unet: Trained model\n",
        "    :params schedule: Variance schedule\n",
        "\n",
        "    Returns image at timestep t-1\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(x_t)\n",
        "\n",
        "    beta = linear_variance_schedule(T)\n",
        "    alpha = 1 - beta\n",
        "    alpha_bar = torch.cumprod(alpha, 0)\n",
        "    alpha_bar_prev = torch.cat([torch.tensor([1]).cuda(), alpha_bar[:-1]], 0) # concat 1 in the beginning to match size\n",
        "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar)\n",
        "\n",
        "    # calculations for posterior p(x_{t-1} | x_t, x_0)\n",
        "    variance = ((1 - alpha_bar_prev) / (1 - alpha_bar)) * beta\n",
        "\n",
        "    # Call model (current image - noise prediction)\n",
        "    mean = (1 / torch.sqrt(alpha[t])) * (x_t - (beta[t]/sqrt_one_minus_alpha_bar[t]) * unet(x_t, t))\n",
        "\n",
        "    if t[0] == 0:\n",
        "        return mean\n",
        "    else:\n",
        "        x_prev = mean + torch.sqrt(variance[t]) * noise\n",
        "        return x_prev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading a checkpoint\n",
        "\n",
        "**Note** The UNet model on MNIST dataset is already trained for 40 epochs and saved the checkpoint of the model and now we are loading the saved checkpoint file from that file load the model and optimizer. The training time of the unet model is little huge.\n"
      ],
      "metadata": {
        "id": "-VEEp1Ur5YKt"
      },
      "id": "-VEEp1Ur5YKt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the downloaded checkpoint file path\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath) # Load the saved or downloaded checkpoint\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "4w6v0dCL5a2b"
      },
      "id": "4w6v0dCL5a2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function reads the checkpoint file and loads the downloaded or saved model state and optimizer state to an instance of model and optimizer. Loading a state essentially means is that it sets the model/optimizer parameters to the values as present in the saved checkpoint."
      ],
      "metadata": {
        "id": "4M0vW9YJ5eVl"
      },
      "id": "4M0vW9YJ5eVl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the checkpoint path here\n",
        "ckp_path = \"/content/mnist_model.pth\"\n",
        "\n",
        "# Call the load checkpoint function by passing the file path\n",
        "# Basically, you first initialize your model and optimizer and then update the state dictionaries using the load checkpoint function.\n",
        "unet, optimizer = load_ckp(ckp_path, unet, optimizer)"
      ],
      "metadata": {
        "id": "brYxupQ23C-9"
      },
      "id": "brYxupQ23C-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "086a8bbf",
      "metadata": {
        "id": "086a8bbf"
      },
      "source": [
        "### Visualize reverse diffusion process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ecb2cd",
      "metadata": {
        "id": "82ecb2cd"
      },
      "outputs": [],
      "source": [
        "T = 300\n",
        "x_t = torch.randn(1, 1, IMG_SIZE, IMG_SIZE).to(device)\n",
        "plt.figure(figsize=(25, 25))\n",
        "num_images = 10\n",
        "stepsize = int(T//num_images)\n",
        "\n",
        "for t in range(T-1, -1, -1):\n",
        "    t = torch.full((1,), t, device=device, dtype=torch.long)\n",
        "    # Call the sampling function to visualize the denoised image from t-1 time steps\n",
        "    x_t = sampling(x_t, t, unet)\n",
        "    if t % stepsize == 0:\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1, num_images, int(torch.div(t, stepsize, rounding_mode='trunc'))+1)\n",
        "        plt.imshow(x_t[0,0].detach().cpu().numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ],
      "id": "VHfHdGCP_n6Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mIPZ4gnOcgnN"
      },
      "source": [
        "#@title Q.1. A diffusion probabilistic model (diffusion model) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time.\n",
        "Answer1 = \"TRUE\" #@param [\"\",\"TRUE\", \"FALSE\"]\n"
      ],
      "execution_count": 4,
      "outputs": [],
      "id": "mIPZ4gnOcgnN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNaLFvpbcgm5"
      },
      "source": [
        "#### Consider the following statements about Diffusion Models and answer Q2.\n",
        "\n",
        "\n",
        "A. Diffusion models aim to decompose the image generation process (sampling) in many small “denoising” steps.\n",
        "\n",
        "B. The forward diffusion process takes the input image and gradually adds gaussian noise to it through a series of 'T' time steps according to a variance schedule.\n",
        "\n",
        "C. In the reverse process, a neural network (U-Net) is trained to recover the original image from the gaussian noise by gradually removing the predicted noise at each time step."
      ],
      "id": "dNaLFvpbcgm5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_F3RfGojqZDc"
      },
      "source": [
        "#@title Q.2. Which of the above statements is/are True for Diffusion Models?\n",
        "Answer2 = \"A, B and C\" #@param [\"\",\"Only A\", \"Only C\", \"Only A and B\", \"Only B and C\", \"Only A and C\", \"A, B and C\"]\n"
      ],
      "execution_count": 5,
      "outputs": [],
      "id": "_F3RfGojqZDc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 6,
      "outputs": [],
      "id": "NMzKSbLIgFzQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": [],
      "id": "DjcH1VWSFI2l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 8,
      "outputs": [],
      "id": "4VBk_4VTAxCM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": [],
      "id": "XH91cL1JWH7m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": [],
      "id": "z8xLqj7VWIKW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "3a584fe7-286b-4afc-bfbb-5dd42ab7850a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2906\n",
            "Date of submission:  08 Oct 2023\n",
            "Time of submission:  16:47:20\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "id": "FzAZHt1zw-Y-"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}